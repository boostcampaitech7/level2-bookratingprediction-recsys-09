{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import sys\n",
    "import yaml\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def basic_data_load(args):\n",
    "\t\"\"\"\n",
    "\tParameters\n",
    "\t----------\n",
    "\targs.dataset.data_path : str\n",
    "\t\t데이터 경로를 설정할 수 있는 parser\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tdata : dict\n",
    "\t\t학습 및 테스트 데이터가 담긴 사전 형식의 데이터를 반환합니다\n",
    "\t\"\"\"\n",
    "\n",
    "\t######################## DATA LOAD\n",
    "\t# users = pd.read_csv(args.dataset.data_path + 'users.csv')\n",
    "\t# books = pd.read_csv(args.dataset.data_path + 'books.csv')\n",
    "\ttrain_df = pd.read_csv(args.dataset.data_path + 'train_ratings.csv')\n",
    "\ttest_df = pd.read_csv(args.dataset.data_path + 'test_ratings.csv')\n",
    "\tsub = pd.read_csv(args.dataset.data_path + 'sample_submission.csv')\n",
    "\n",
    "\tall_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "\tsparse_cols = ['user_id', 'isbn']\n",
    "\n",
    "\t# 라벨 인코딩하고 인덱스 정보를 저장\n",
    "\tlabel2idx, idx2label = {}, {}\n",
    "\tfor col in sparse_cols:\n",
    "\t\tall_df[col] = all_df[col].fillna('unknown')\n",
    "\t\tunique_labels = all_df[col].astype(\"category\").cat.categories\n",
    "\t\tlabel2idx[col] = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\t\tidx2label[col] = {idx: label for idx, label in enumerate(unique_labels)}\n",
    "\t\ttrain_df[col] = train_df[col].map(label2idx[col])\n",
    "\t\ttest_df[col] = test_df[col].map(label2idx[col])\n",
    "\n",
    "\tfield_dims = [len(label2idx[col]) for col in sparse_cols]\n",
    "\n",
    "\t# 각 유저별 평점 평균을 구하고 train_df, test_df에 추가\n",
    "\tuser_rating_mean = train_df.groupby('user_id')['rating'].mean()\n",
    "\ttrain_df['user_rating_mean'] = train_df.user_id.map(user_rating_mean)\n",
    "\ttest_df['user_rating_mean'] = test_df.user_id.map(user_rating_mean)\n",
    "\n",
    "\t# 각 유저의 평점 개수를 구하고 train_df,  test_df에 추가\n",
    "\tuser_rating_count = train_df['user_id'].value_counts()\n",
    "\ttrain_df['user_rating_count'] = train_df.user_id.map(user_rating_count)\n",
    "\ttest_df['user_rating_count'] = test_df.user_id.map(user_rating_count)\n",
    "\t# train_df에 없던 새로운 유저의 경우 평점 개수를 0으로 설정\n",
    "\ttest_df['user_rating_count'] = test_df['user_rating_count'].fillna(0)\n",
    "\n",
    "\t# isbn 별 평점 평균을 구하고 train_df, test_df에 추가\n",
    "\tbook_rating_mean = train_df.groupby('isbn')['rating'].mean()\n",
    "\ttrain_df['book_rating_mean'] = train_df.isbn.map(book_rating_mean)\n",
    "\ttest_df['book_rating_mean'] = test_df.isbn.map(book_rating_mean)\n",
    "\n",
    "\t# isbn 별 평점 평균과의 차이를 구하고 train_df에 추가\n",
    "\ttrain_df['diff_book_rating_mean'] = train_df['rating'] - train_df['book_rating_mean']\n",
    "\n",
    "\tdata = {\n",
    "\t\t'train': train_df,\n",
    "\t\t'test': test_df.drop(['rating'], axis=1),\n",
    "\t\t'field_dims': field_dims,\n",
    "\t\t'label2idx': label2idx,\n",
    "\t\t'idx2label': idx2label,\n",
    "\t\t'sub': sub,\n",
    "\t}\n",
    "\n",
    "\treturn data\n",
    "\n",
    "\n",
    "def basic_data_split(args, data):\n",
    "\t\"\"\"\n",
    "\tParameters\n",
    "\t----------\n",
    "\targs.dataset.valid_ratio : float\n",
    "\t\tTrain/Valid split 비율을 입력합니다.\n",
    "\targs.seed : int\n",
    "\t\t데이터 셔플 시 사용할 seed 값을 입력합니다.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tdata : dict\n",
    "\t\tdata 내의 학습 데이터를 학습/검증 데이터로 나누어 추가한 후 반환합니다.\n",
    "\t\"\"\"\n",
    "\tif args.dataset.valid_ratio == 0:\n",
    "\t\tdata['X_train'] = data['train'].drop('rating', axis=1)\n",
    "\t\tdata['y_train'] = data['train']['rating']\n",
    "\n",
    "\telse:\n",
    "\t\tX_train, X_valid, y_train, y_valid = train_test_split(\n",
    "\t\t\tdata['train'].drop(['rating'], axis=1),\n",
    "\t\t\tdata['train']['rating'],\n",
    "\t\t\ttest_size=args.dataset.valid_ratio,\n",
    "\t\t\trandom_state=args.seed,\n",
    "\t\t\tshuffle=True\n",
    "\t\t)\n",
    "\t\tdata['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid\n",
    "\n",
    "\treturn data\n",
    "\n",
    "\n",
    "def basic_data_loader(args, data):\n",
    "\t\"\"\"\n",
    "\tParameters\n",
    "\t----------\n",
    "\targs.dataloader.batch_size : int\n",
    "\t\t데이터 batch에 사용할 데이터 사이즈\n",
    "\targs.dataloader.shuffle : bool\n",
    "\t\tdata shuffle 여부\n",
    "\targs.dataloader.num_workers: int\n",
    "\t\tdataloader에서 사용할 멀티프로세서 수\n",
    "\targs.dataset.valid_ratio : float\n",
    "\t\tTrain/Valid split 비율로, 0일 경우에 대한 처리를 위해 사용합니다.\n",
    "\tdata : dict\n",
    "\t\tbasic_data_split 함수에서 반환된 데이터\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tdata : dict\n",
    "\t\tDataLoader가 추가된 데이터를 반환합니다.\n",
    "\t\"\"\"\n",
    "\n",
    "\ttrain_dataset = TensorDataset(torch.LongTensor(data['X_train'].values), torch.LongTensor(data['y_train'].values))\n",
    "\tvalid_dataset = TensorDataset(torch.LongTensor(data['X_valid'].values),\n",
    "\t\t\t\t\t\t\t\t  torch.LongTensor(data['y_valid'].values)) if args.dataset.valid_ratio != 0 else None\n",
    "\ttest_dataset = TensorDataset(torch.LongTensor(data['test'].values))\n",
    "\n",
    "\ttrain_dataloader = DataLoader(train_dataset, batch_size=args.dataloader.batch_size, shuffle=args.dataloader.shuffle,\n",
    "\t\t\t\t\t\t\t\t  num_workers=args.dataloader.num_workers)\n",
    "\tvalid_dataloader = DataLoader(valid_dataset, batch_size=args.dataloader.batch_size, shuffle=False,\n",
    "\t\t\t\t\t\t\t\t  num_workers=args.dataloader.num_workers) if args.dataset.valid_ratio != 0 else None\n",
    "\ttest_dataloader = DataLoader(test_dataset, batch_size=args.dataloader.batch_size, shuffle=False,\n",
    "\t\t\t\t\t\t\t\t num_workers=args.dataloader.num_workers)\n",
    "\n",
    "\tdata['train_dataloader'], data['valid_dataloader'], data[\n",
    "\t\t'test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader\n",
    "\n",
    "\treturn data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def str2list(x: str) -> list:\n",
    "\t'''문자열을 리스트로 변환하는 함수'''\n",
    "\treturn x[1:-1].split(', ')\n",
    "\n",
    "\n",
    "def split_location(x: str) -> list:\n",
    "\t'''\n",
    "\tParameters\n",
    "\t----------\n",
    "\tx : str\n",
    "\t\tlocation 데이터\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tres : list\n",
    "\t\tlocation 데이터를 나눈 뒤, 정제한 결과를 반환합니다.\n",
    "\t\t순서는 country, state, city, ... 입니다.\n",
    "\t'''\n",
    "\tres = x.split(',')\n",
    "\tres = [i.strip().lower() for i in res]\n",
    "\tres = [regex.sub(r'[^a-zA-Z/ ]', '', i) for i in res]  # remove special characters\n",
    "\tres = [i if i not in ['n/a', ''] else np.nan for i in res]  # change 'n/a' into NaN\n",
    "\tres.reverse()  # reverse the list to get country, state, city, ... order\n",
    "\n",
    "\tfor i in range(len(res) - 1, 0, -1):\n",
    "\t\tif (res[i] in res[:i]) and (not pd.isna(res[i])):  # remove duplicated values if not NaN\n",
    "\t\t\tres.pop(i)\n",
    "\n",
    "\treturn res\n",
    "\n",
    "def process_context_data(users, books):\n",
    "\t\"\"\"\n",
    "\tParameters\n",
    "\t----------\n",
    "\tusers : pd.DataFrame\n",
    "\t\tusers.csv를 인덱싱한 데이터\n",
    "\tbooks : pd.DataFrame\n",
    "\t\tbooks.csv를 인덱싱한 데이터\n",
    "\tratings1 : pd.DataFrame\n",
    "\t\ttrain 데이터의 rating\n",
    "\tratings2 : pd.DataFrame\n",
    "\t\ttest 데이터의 rating\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tlabel_to_idx : dict\n",
    "\t\t데이터를 인덱싱한 정보를 담은 딕셔너리\n",
    "\tidx_to_label : dict\n",
    "\t\t인덱스를 다시 원래 데이터로 변환하는 정보를 담은 딕셔너리\n",
    "\ttrain_df : pd.DataFrame\n",
    "\t\ttrain 데이터\n",
    "\ttest_df : pd.DataFrame\n",
    "\t\ttest 데이터\n",
    "\t\"\"\"\n",
    "\n",
    "\tusers_ = users.copy()\n",
    "\tbooks_ = books.copy()\n",
    "\n",
    "\t# 데이터 전처리 (전처리는 각자의 상황에 맞게 진행해주세요!)\n",
    "\tbooks_['category'] = books_['category'].apply(lambda x: str2list(x)[0] if not pd.isna(x) else np.nan)\n",
    "\tbooks_['language'] = books_['language'].fillna(books_['language'].mode()[0])\n",
    "\tbooks_['publication_range'] = books_['year_of_publication'].apply(\n",
    "\t\tlambda x: x // 10 * 10)  # 1990년대, 2000년대, 2010년대, ...\n",
    "\n",
    "\tusers_['age'] = users_['age'].fillna(users_['age'].mode()[0])\n",
    "\tusers_['age_range'] = users_['age'].apply(lambda x: x // 10 * 10)  # 10대, 20대, 30대, ...\n",
    "\n",
    "\tusers_['location_list'] = users_['location'].apply(lambda x: split_location(x))\n",
    "\tusers_['location_country'] = users_['location_list'].apply(lambda x: x[0])\n",
    "\tusers_['location_state'] = users_['location_list'].apply(lambda x: x[1] if len(x) > 1 else np.nan)\n",
    "\tusers_['location_city'] = users_['location_list'].apply(lambda x: x[2] if len(x) > 2 else np.nan)\n",
    "\tfor idx, row in users_.iterrows():\n",
    "\t\tif (not pd.isna(row['location_state'])) and pd.isna(row['location_country']):\n",
    "\t\t\tfill_country = users_[users_['location_state'] == row['location_state']]['location_country'].mode()\n",
    "\t\t\tfill_country = fill_country[0] if len(fill_country) > 0 else np.nan\n",
    "\t\t\tusers_.loc[idx, 'location_country'] = fill_country\n",
    "\t\telif (not pd.isna(row['location_city'])) and pd.isna(row['location_state']):\n",
    "\t\t\tif not pd.isna(row['location_country']):\n",
    "\t\t\t\tfill_state = users_[(users_['location_country'] == row['location_country'])\n",
    "\t\t\t\t\t\t\t\t\t& (users_['location_city'] == row['location_city'])]['location_state'].mode()\n",
    "\t\t\t\tfill_state = fill_state[0] if len(fill_state) > 0 else np.nan\n",
    "\t\t\t\tusers_.loc[idx, 'location_state'] = fill_state\n",
    "\t\t\telse:\n",
    "\t\t\t\tfill_state = users_[users_['location_city'] == row['location_city']]['location_state'].mode()\n",
    "\t\t\t\tfill_state = fill_state[0] if len(fill_state) > 0 else np.nan\n",
    "\t\t\t\tfill_country = users_[users_['location_city'] == row['location_city']]['location_country'].mode()\n",
    "\t\t\t\tfill_country = fill_country[0] if len(fill_country) > 0 else np.nan\n",
    "\t\t\t\tusers_.loc[idx, 'location_country'] = fill_country\n",
    "\t\t\t\tusers_.loc[idx, 'location_state'] = fill_state\n",
    "\n",
    "\tusers_ = users_.drop(['location'], axis=1)\n",
    "\n",
    "\treturn users_, books_\n",
    "\n",
    "\n",
    "def context_data_load(args):\n",
    "\t\"\"\"\n",
    "\tParameters\n",
    "\t----------\n",
    "\targs.dataset.data_path : str\n",
    "\t\t데이터 경로를 설정할 수 있는 parser\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tdata : dict\n",
    "\t\t학습 및 테스트 데이터가 담긴 사전 형식의 데이터를 반환합니다.\n",
    "\t\"\"\"\n",
    "\n",
    "\t######################## DATA LOAD\n",
    "\tusers = pd.read_csv(args.dataset.data_path + 'users.csv')\n",
    "\tbooks = pd.read_csv(args.dataset.data_path + 'books.csv')\n",
    "\ttrain = pd.read_csv(args.dataset.data_path + 'train_ratings.csv')\n",
    "\ttest = pd.read_csv(args.dataset.data_path + 'test_ratings.csv')\n",
    "\tsub = pd.read_csv(args.dataset.data_path + 'sample_submission.csv')\n",
    "\n",
    "\tusers_, books_ = process_context_data(users, books)\n",
    "\n",
    "\t# 유저 및 책 정보를 합쳐서 데이터 프레임 생성\n",
    "\t# 사용할 컬럼을 user_features와 book_features에 정의합니다. (단, 모두 범주형 데이터로 가정)\n",
    "\t# 베이스라인에서는 가능한 모든 컬럼을 사용하도록 구성하였습니다.\n",
    "\t# NCF를 사용할 경우, idx 0, 1은 각각 user_id, isbn이어야 합니다.\n",
    "\tuser_features = ['user_id', 'age_range', 'location_country', 'location_state', 'location_city']\n",
    "\tbook_features = ['isbn', 'book_title', 'book_author', 'publisher', 'language', 'category', 'publication_range']\n",
    "\tsparse_cols = ['user_id', 'isbn'] + list(\n",
    "\t\tset(user_features + book_features) - {'user_id', 'isbn'}) if args.model == 'NCF' \\\n",
    "\t\telse user_features + book_features\n",
    "\n",
    "\t# 선택한 컬럼만 추출하여 데이터 조인\n",
    "\ttrain_df = train.merge(users_, on='user_id', how='left') \\\n",
    "\t\t.merge(books_, on='isbn', how='left')[sparse_cols + ['rating']]\n",
    "\ttest_df = test.merge(users_, on='user_id', how='left') \\\n",
    "\t\t.merge(books_, on='isbn', how='left')[sparse_cols]\n",
    "\tall_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "\t# feature_cols의 데이터만 라벨 인코딩하고 인덱스 정보를 저장\n",
    "\tlabel2idx, idx2label = {}, {}\n",
    "\tfor col in sparse_cols:\n",
    "\t\tall_df[col] = all_df[col].fillna('unknown')\n",
    "\t\tunique_labels = all_df[col].astype(\"category\").cat.categories\n",
    "\t\tlabel2idx[col] = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\t\tidx2label[col] = {idx: label for idx, label in enumerate(unique_labels)}\n",
    "\t\ttrain_df[col] = train_df[col].map(label2idx[col])\n",
    "\t\ttest_df[col] = test_df[col].map(label2idx[col])\n",
    "\n",
    "\tfield_dims = [len(label2idx[col]) for col in train_df.columns if col != 'rating']\n",
    "\n",
    "\tdata = {\n",
    "\t\t'train': train_df,\n",
    "\t\t'test': test_df,\n",
    "\t\t'field_names': sparse_cols,\n",
    "\t\t'field_dims': field_dims,\n",
    "\t\t'label2idx': label2idx,\n",
    "\t\t'idx2label': idx2label,\n",
    "\t\t'sub': sub,\n",
    "\t}\n",
    "\n",
    "\treturn data\n",
    "\n",
    "\n",
    "def context_data_split(args, data):\n",
    "\t'''data 내의 학습 데이터를 학습/검증 데이터로 나누어 추가한 후 반환합니다.'''\n",
    "\treturn basic_data_split(args, data)\n",
    "\n",
    "\n",
    "def context_data_loader(args, data):\n",
    "\t\"\"\"\n",
    "\tParameters\n",
    "\t----------\n",
    "\targs.dataloader.batch_size : int\n",
    "\t\t데이터 batch에 사용할 데이터 사이즈\n",
    "\targs.dataloader.shuffle : bool\n",
    "\t\tdata shuffle 여부\n",
    "\targs.dataloader.num_workers: int\n",
    "\t\tdataloader에서 사용할 멀티프로세서 수\n",
    "\targs.dataset.valid_ratio : float\n",
    "\t\tTrain/Valid split 비율로, 0일 경우에 대한 처리를 위해 사용합니다.\n",
    "\tdata : dict\n",
    "\t\tcontext_data_load 함수에서 반환된 데이터\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tdata : dict\n",
    "\t\tDataLoader가 추가된 데이터를 반환합니다.\n",
    "\t\"\"\"\n",
    "\n",
    "\ttrain_dataset = TensorDataset(torch.LongTensor(data['X_train'].values), torch.LongTensor(data['y_train'].values))\n",
    "\tvalid_dataset = TensorDataset(torch.LongTensor(data['X_valid'].values),\n",
    "\t\t\t\t\t\t\t\t  torch.LongTensor(data['y_valid'].values)) if args.dataset.valid_ratio != 0 else None\n",
    "\ttest_dataset = TensorDataset(torch.LongTensor(data['test'].values))\n",
    "\n",
    "\ttrain_dataloader = DataLoader(train_dataset, batch_size=args.dataloader.batch_size, shuffle=args.dataloader.shuffle,\n",
    "\t\t\t\t\t\t\t\t  num_workers=args.dataloader.num_workers)\n",
    "\tvalid_dataloader = DataLoader(valid_dataset, batch_size=args.dataloader.batch_size, shuffle=False,\n",
    "\t\t\t\t\t\t\t\t  num_workers=args.dataloader.num_workers) if args.dataset.valid_ratio != 0 else None\n",
    "\ttest_dataloader = DataLoader(test_dataset, batch_size=args.dataloader.batch_size, shuffle=False,\n",
    "\t\t\t\t\t\t\t\t num_workers=args.dataloader.num_workers)\n",
    "\n",
    "\tdata['train_dataloader'], data['valid_dataloader'], data[\n",
    "\t\t'test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader\n",
    "\n",
    "\treturn data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/boostcamp11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def text_preprocessing(summary):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary : pd.Series\n",
    "        정규화와 같은 기본적인 전처리를 하기 위한 텍스트 데이터를 입력합니다.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    summary : pd.Series\n",
    "        전처리된 텍스트 데이터를 반환합니다.\n",
    "        베이스라인에서는 특수문자 제거, 공백 제거를 진행합니다.\n",
    "    \"\"\"\n",
    "    summary = re.sub(\"[^0-9a-zA-Z.,!?]\", \" \", summary)  # .,!?를 제외한 특수문자 제거\n",
    "    summary = re.sub(\"\\s+\", \" \", summary)  # 중복 공백 제거\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def text_to_vector(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        `summary_merge()`를 통해 병합된 요약 데이터\n",
    "    tokenizer : Tokenizer\n",
    "        텍스트 데이터를 `model`에 입력하기 위한 토크나이저\n",
    "    model : 사전학습된 언어 모델\n",
    "        텍스트 데이터를 벡터로 임베딩하기 위한 모델\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    text_ = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized = tokenizer.encode(text_, add_special_tokens=True)\n",
    "    token_tensor = torch.tensor([tokenized], device=model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_tensor)  # attention_mask를 사용하지 않아도 됨\n",
    "        ### BERT 모델의 경우, 최종 출력물의 사이즈가 (토큰길이, 임베딩=768)이므로, 이를 평균내어 사용하거나 pooler_output을 사용하여 [CLS] 토큰의 임베딩만 사용\n",
    "        # sentence_embedding = torch.mean(outputs.last_hidden_state[0], dim=0)  # 방법1) 모든 토큰의 임베딩을 평균내어 사용\n",
    "        sentence_embedding = outputs.pooler_output.squeeze(0)  # 방법2) pooler_output을 사용하여 맨 첫 토큰인 [CLS] 토큰의 임베딩만 사용\n",
    "    \n",
    "    return sentence_embedding.cpu().detach().numpy() \n",
    "\n",
    "\n",
    "def process_text_data(ratings, users, books, tokenizer, model, vector_create=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    users : pd.DataFrame\n",
    "        유저 정보에 대한 데이터 프레임을 입력합니다.\n",
    "    books : pd.DataFrame\n",
    "        책 정보에 대한 데이터 프레임을 입력합니다.\n",
    "    vector_create : bool\n",
    "        사전에 텍스트 데이터 벡터화가 된 파일이 있는지 여부를 입력합니다.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `users_` : pd.DataFrame\n",
    "        각 유저가 읽은 책에 대한 요약 정보를 병합 및 벡터화하여 추가한 데이터 프레임을 반환합니다.\n",
    "\n",
    "    `books_` : pd.DataFrame\n",
    "        텍스트 데이터를 벡터화하여 추가한 데이터 프레임을 반환합니다.\n",
    "    \"\"\"\n",
    "    num2txt = ['Zero', 'One', 'Two', 'Three', 'Four', 'Five']\n",
    "    users_ = users.copy()\n",
    "    books_ = books.copy()\n",
    "    nan_value = 'None'\n",
    "    books_['summary'] = books_['summary'].fillna(nan_value)\\\n",
    "                                         .apply(lambda x: text_preprocessing(x))\\\n",
    "                                         .replace({'': nan_value, ' ': nan_value})\n",
    "    \n",
    "    books_['summary_length'] = books_['summary'].apply(lambda x:len(x))\n",
    "    books_['review_count'] = books_['isbn'].map(ratings['isbn'].value_counts())\n",
    "\n",
    "    users_['books_read'] = users_['user_id'].map(ratings.groupby('user_id')['isbn'].apply(list))\n",
    "\n",
    "    if vector_create:\n",
    "        if not os.path.exists('./data/text_vector'):\n",
    "            os.makedirs('./data/text_vector')\n",
    "\n",
    "        print('Create Item Summary Vector')\n",
    "        book_summary_vector_list = []\n",
    "        for title, summary in tqdm(zip(books_['book_title'], books_['summary']), total=len(books_)):\n",
    "            # 책에 대한 텍스트 프롬프트는 아래와 같이 구성됨\n",
    "            # '''\n",
    "            # Book Title: {title}\n",
    "            # Summary: {summary}\n",
    "            # '''\n",
    "            prompt_ = f'Book Title: {title}\\n Summary: {summary}\\n'\n",
    "            vector = text_to_vector(prompt_, tokenizer, model)\n",
    "            book_summary_vector_list.append(vector)\n",
    "        \n",
    "        book_summary_vector_list = np.concatenate([\n",
    "                                                books_['isbn'].values.reshape(-1, 1),\n",
    "                                                np.asarray(book_summary_vector_list, dtype=np.float32)\n",
    "                                                ], axis=1)\n",
    "        \n",
    "        np.save('./data/text_vector/book_summary_vector.npy', book_summary_vector_list)\n",
    "\n",
    "        print('Create User Summary Merge Vector')\n",
    "\n",
    "        user_summary_merge_vector_list = []\n",
    "        for books_read in tqdm(users_['books_read']):\n",
    "            if not isinstance(books_read, list) and pd.isna(books_read):  # 유저가 읽은 책이 없는 경우, 텍스트 임베딩을 0으로 처리\n",
    "                user_summary_merge_vector_list.append(np.zeros((768)))\n",
    "                continue\n",
    "            \n",
    "            read_books = books_[books_['isbn'].isin(books_read)][['book_title', 'summary', 'review_count']]\n",
    "            read_books = read_books.sort_values('review_count', ascending=False).head(5)  # review_count가 높은 순으로 5개의 책을 선택\n",
    "            # 유저에 대한 텍스트 프롬프트는 아래와 같이 구성됨\n",
    "            # DeepCoNN에서 유저의 리뷰를 요약하여 하나의 벡터로 만들어 사용함을 참고 (https://arxiv.org/abs/1701.04783)\n",
    "            # '''\n",
    "            # Five Books That You Read\n",
    "            # 1. Book Title: {title}\n",
    "            # Summary: {summary}\n",
    "            # ...\n",
    "            # 5. Book Title: {title}\n",
    "            # Summary: {summary}\n",
    "            # '''\n",
    "            prompt_ = f'{num2txt[len(read_books)]} Books That You Read\\n'\n",
    "            for idx, (title, summary) in enumerate(zip(read_books['book_title'], read_books['summary'])):\n",
    "                summary = summary if len(summary) < 100 else f'{summary[:100]} ...'\n",
    "                prompt_ += f'{idx+1}. Book Title: {title}\\n Summary: {summary}\\n'\n",
    "            vector = text_to_vector(prompt_, tokenizer, model)\n",
    "            user_summary_merge_vector_list.append(vector)\n",
    "        \n",
    "        user_summary_merge_vector_list = np.concatenate([\n",
    "                                                         users_['user_id'].values.reshape(-1, 1),\n",
    "                                                         np.asarray(user_summary_merge_vector_list, dtype=np.float32)\n",
    "                                                        ], axis=1)\n",
    "        \n",
    "        np.save('./data/text_vector/user_summary_merge_vector.npy', user_summary_merge_vector_list)        \n",
    "        \n",
    "    else:\n",
    "        print('Check Vectorizer')\n",
    "        print('Vector Load')\n",
    "        book_summary_vector_list = np.load('./data/text_vector/book_summary_vector.npy', allow_pickle=True)\n",
    "        user_summary_merge_vector_list = np.load('./data/text_vector/user_summary_merge_vector.npy', allow_pickle=True)\n",
    "\n",
    "    book_summary_vector_df = pd.DataFrame({'isbn': book_summary_vector_list[:, 0]})\n",
    "    book_summary_vector_df['book_summary_vector'] = list(book_summary_vector_list[:, 1:].astype(np.float32))\n",
    "    user_summary_vector_df = pd.DataFrame({'user_id': user_summary_merge_vector_list[:, 0]})\n",
    "    user_summary_vector_df['user_summary_merge_vector'] = list(user_summary_merge_vector_list[:, 1:].astype(np.float32))\n",
    "\n",
    "    books_ = pd.merge(books_, book_summary_vector_df, on='isbn', how='left')\n",
    "    users_ = pd.merge(users_, user_summary_vector_df, on='user_id', how='left')\n",
    "\n",
    "    return users_, books_\n",
    "\n",
    "\n",
    "class Text_Dataset(Dataset):\n",
    "    def __init__(self, user_book_vector, user_summary_vector, book_summary_vector, rating=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_book_vector : np.ndarray\n",
    "            벡터화된 유저와 책 데이터를 입렵합니다.\n",
    "        user_summary_vector : np.ndarray\n",
    "            벡터화된 유저에 대한 요약 정보 데이터를 입력합니다.\n",
    "        book_summary_vector : np.ndarray\n",
    "            벡터화된 책에 대한 요약 정보 데이터 입력합니다.\n",
    "        label : np.ndarray\n",
    "            정답 데이터를 입력합니다.\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.user_book_vector = user_book_vector\n",
    "        self.user_summary_vector = user_summary_vector\n",
    "        self.book_summary_vector = book_summary_vector\n",
    "        self.rating = rating\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_book_vector.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "                'user_book_vector' : torch.tensor(self.user_book_vector[i], dtype=torch.long),\n",
    "                'user_summary_vector' : torch.tensor(self.user_summary_vector[i], dtype=torch.float32),\n",
    "                'book_summary_vector' : torch.tensor(self.book_summary_vector[i], dtype=torch.float32),\n",
    "                'rating' : torch.tensor(self.rating[i], dtype=torch.float32),\n",
    "                } if self.rating is not None else \\\n",
    "                {\n",
    "                'user_book_vector' : torch.tensor(self.user_book_vector[i], dtype=torch.long),\n",
    "                'user_summary_vector' : torch.tensor(self.user_summary_vector[i], dtype=torch.float32),\n",
    "                'book_summary_vector' : torch.tensor(self.book_summary_vector[i], dtype=torch.float32),\n",
    "                }\n",
    "\n",
    "\n",
    "def text_data_load(args):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    args.dataset.data_path : str\n",
    "        데이터 경로를 설정할 수 있는 parser\n",
    "    args.model_args[args.model].pretrained_model : str\n",
    "        사전학습된 모델을 설정할 수 있는 parser\n",
    "    args.model_args[args.model].vector_create : bool\n",
    "        텍스트 데이터 벡터화 및 저장 여부를 설정할 수 있는 parser\n",
    "        False로 설정하면 기존에 저장된 벡터를 불러옵니다.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        학습 및 테스트 데이터가 담긴 사전 형식의 데이터를 반환합니다.\n",
    "    \"\"\"\n",
    "    users = pd.read_csv(args.dataset.data_path + 'users.csv')\n",
    "    books = pd.read_csv(args.dataset.data_path + 'books.csv')\n",
    "    train = pd.read_csv(args.dataset.data_path + 'train_ratings.csv')\n",
    "    test = pd.read_csv(args.dataset.data_path + 'test_ratings.csv')\n",
    "    sub = pd.read_csv(args.dataset.data_path + 'sample_submission.csv')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_args[args.model].pretrained_model)\n",
    "    model = AutoModel.from_pretrained(args.model_args[args.model].pretrained_model).to(device=args.device)\n",
    "    model.eval()\n",
    "    users_, books_ = process_text_data(train, users, books, tokenizer, model, args.model_args[args.model].vector_create)\n",
    "\n",
    "    # 유저 및 책 정보를 합쳐서 데이터 프레임 생성 (단, 베이스라인에서는 user_id, isbn, user_summary_merge_vector, book_summary_vector만 사용함)\n",
    "    # 사용할 컬럼을 user_features와 book_features에 정의합니다. (단, 모두 범주형 데이터로 가정)\n",
    "    user_features = []\n",
    "    book_features = []\n",
    "    sparse_cols = ['user_id', 'isbn'] + list(set(user_features + book_features) - {'user_id', 'isbn'})\n",
    "    \n",
    "    train_df = train.merge(books_, on='isbn', how='left')\\\n",
    "                    .merge(users_, on='user_id', how='left')[sparse_cols + ['user_summary_merge_vector', 'book_summary_vector', 'rating']]\n",
    "    test_df = test.merge(books_, on='isbn', how='left')\\\n",
    "                  .merge(users_, on='user_id', how='left')[sparse_cols + ['user_summary_merge_vector', 'book_summary_vector']]\n",
    "    all_df = pd.concat([train, test], axis=0)\n",
    "\n",
    "    # feature_cols의 데이터만 라벨 인코딩하고 인덱스 정보를 저장\n",
    "    label2idx, idx2label = {}, {}\n",
    "    for col in sparse_cols:\n",
    "        all_df[col] = all_df[col].fillna('unknown')\n",
    "        unique_labels = all_df[col].astype(\"category\").cat.categories\n",
    "        label2idx[col] = {label:idx for idx, label in enumerate(unique_labels)}\n",
    "        idx2label[col] = {idx:label for idx, label in enumerate(unique_labels)}\n",
    "        train_df[col] = train_df[col].map(label2idx[col])\n",
    "        test_df[col] = test_df[col].map(label2idx[col])\n",
    "\n",
    "    field_dims = [len(label2idx[col]) for col in sparse_cols]\n",
    "\n",
    "    data = {\n",
    "            'train':train_df,\n",
    "            'test':test_df,\n",
    "            'field_names':sparse_cols,\n",
    "            'field_dims':field_dims,\n",
    "            'label2idx':label2idx,\n",
    "            'idx2label':idx2label,\n",
    "            'sub':sub,\n",
    "            }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def text_data_split(args, data):\n",
    "    \"\"\"학습 데이터를 학습/검증 데이터로 나누어 추가한 후 반환합니다.\"\"\"\n",
    "    return basic_data_split(args, data)\n",
    "\n",
    "\n",
    "def text_data_loader(args, data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    args.dataloader.batch_size : int\n",
    "        데이터 batch에 사용할 데이터 사이즈\n",
    "    args.dataloader.shuffle : bool\n",
    "        data shuffle 여부\n",
    "    args.dataloader.num_workers: int\n",
    "        dataloader에서 사용할 멀티프로세서 수\n",
    "    args.dataset.valid_ratio : float\n",
    "        Train/Valid split 비율로, 0일 경우에 대한 처리를 위해 사용\n",
    "    data : dict\n",
    "        text_data_load()에서 반환된 데이터\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        Text_Dataset 형태의 학습/검증/테스트 데이터를 DataLoader로 변환하여 추가한 후 반환합니다.\n",
    "    \"\"\"\n",
    "    train_dataset = Text_Dataset(\n",
    "                                data['X_train'][data['field_names']].values,\n",
    "                                data['X_train']['user_summary_merge_vector'].values,\n",
    "                                data['X_train']['book_summary_vector'].values,\n",
    "                                data['y_train'].values\n",
    "                                )\n",
    "    valid_dataset = Text_Dataset(\n",
    "                                data['X_valid'][data['field_names']].values,\n",
    "                                data['X_valid']['user_summary_merge_vector'].values,\n",
    "                                data['X_valid']['book_summary_vector'].values,\n",
    "                                data['y_valid'].values\n",
    "                                ) if args.dataset.valid_ratio != 0 else None\n",
    "    test_dataset = Text_Dataset(\n",
    "                                data['test'][data['field_names']].values,\n",
    "                                data['test']['user_summary_merge_vector'].values,\n",
    "                                data['test']['book_summary_vector'].values,\n",
    "                                )\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.dataloader.batch_size, shuffle=args.dataloader.shuffle, num_workers=args.dataloader.num_workers)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=args.dataloader.batch_size, shuffle=False, num_workers=args.dataloader.num_workers) if args.dataset.valid_ratio != 0 else None\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.dataloader.batch_size, shuffle=False, num_workers=args.dataloader.num_workers)\n",
    "    data['train_dataloader'], data['valid_dataloader'], data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.data_path: data/\n",
      "dataset.valid_ratio: 0.2\n",
      "dataloader.batch_size: 1024\n",
      "optimizer.type: Adam\n",
      "model_args.FM.embed_dim: 16\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "# YAML 파일을 불러오는 함수\n",
    "def load_config(file_path):\n",
    "    config = OmegaConf.load(file_path)\n",
    "    return config\n",
    "\n",
    "# config.yaml 파일을 불러와서 계층형으로 접근할 수 있게 함\n",
    "config = load_config('./config/config_baseline.yaml')\n",
    "\n",
    "# 예제 출력\n",
    "print(f'dataset.data_path: {config.dataset.data_path}')\n",
    "print(f'dataset.valid_ratio: {config.dataset.valid_ratio}')\n",
    "print(f'dataloader.batch_size: {config.dataloader.batch_size}')\n",
    "print(f'optimizer.type: {config.optimizer.type}')\n",
    "print(f'model_args.FM.embed_dim: {config.model_args.FM.embed_dim}')\n",
    "\n",
    "config.model = 'Text_DeepFM'\n",
    "config.model_args[config.model].vector_create = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Vectorizer\n",
      "Vector Load\n"
     ]
    }
   ],
   "source": [
    "data = text_data_load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text_data_split(config, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_valid = data['X_valid']\n",
    "y_valid = data['y_valid']\n",
    "X_test = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_train = pd.concat([X_train, X_valid], axis=0)\n",
    "y_for_train = pd.concat([y_train, y_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train과 y_train을 병합\n",
    "train_df = X_train.copy()\n",
    "train_df['rating'] = y_train\n",
    "\n",
    "# 동일한 isbn을 가진 데이터의 rating 평균을 구하여 저장\n",
    "isbn_rating_mean = train_df.groupby('isbn')['rating'].mean()\n",
    "X_train['isbn_rating_mean'] = X_train['isbn'].map(isbn_rating_mean)\n",
    "\n",
    "# 동일한 isbn을 가진 데이터의 rating 평균과의 차이를 구g함\n",
    "isbn_diff = y_train - X_train['isbn_rating_mean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train과 y_train을 병합\n",
    "train_df = X_for_train.copy()\n",
    "train_df['rating'] = y_for_train\n",
    "\n",
    "# 동일한 isbn을 가진 데이터의 rating 평균을 구하여 저장\n",
    "isbn_rating_mean = train_df.groupby('isbn')['rating'].mean()\n",
    "X_for_train['isbn_rating_mean'] = X_for_train['isbn'].map(isbn_rating_mean)\n",
    "\n",
    "# 동일한 isbn을 가진 데이터의 rating 평균과의 차이를 구g함\n",
    "isbn_diff_for_train = y_for_train - X_for_train['isbn_rating_mean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train의 값을 isbn_diff로 대체\n",
    "y_train_diff = isbn_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff_for_train = isbn_diff_for_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id    isbn                          user_summary_merge_vector  \\\n",
      "72064     64583   25962  [-0.8408327, -0.4360084, -0.90918756, 0.760248...   \n",
      "165810    43626   54250  [-0.84706074, -0.39163983, -0.8053944, 0.6616,...   \n",
      "268428    32495  118527  [-0.8577756, -0.4652395, -0.92259413, 0.695843...   \n",
      "41373     24096   29368  [-0.8388007, -0.42792618, -0.81633306, 0.69966...   \n",
      "172673    54540   18616  [-0.5625498, -0.22993332, -0.8351104, 0.334444...   \n",
      "...         ...     ...                                                ...   \n",
      "296689    53998  103937  [-0.74025136, -0.47905624, -0.9157893, 0.50602...   \n",
      "120752    32937     651  [-0.75517845, -0.5544599, -0.9751944, 0.714054...   \n",
      "118345    62156   63251  [-0.71679574, -0.36738247, -0.87868947, 0.5782...   \n",
      "43645     13765   24492  [-0.8896811, -0.5306489, -0.9459932, 0.6995699...   \n",
      "202960    44183   77051  [-0.43876028, -0.27600208, -0.884492, 0.275648...   \n",
      "\n",
      "                                      book_summary_vector  isbn_rating_mean  \n",
      "72064   [-0.74674344, -0.42518204, -0.8743399, 0.57239...          7.800000  \n",
      "165810  [-0.9180544, -0.48944014, -0.8480034, 0.792996...          8.000000  \n",
      "268428  [-0.8442989, -0.33626294, -0.41442466, 0.60878...          8.000000  \n",
      "41373   [-0.89569163, -0.4151321, -0.804255, 0.7151592...          5.500000  \n",
      "172673  [-0.9051495, -0.43681535, -0.7584958, 0.737883...          6.500000  \n",
      "...                                                   ...               ...  \n",
      "296689  [-0.8216079, -0.48587963, -0.9546752, 0.695021...          8.000000  \n",
      "120752  [-0.9207887, -0.42884865, -0.8227584, 0.780076...          8.176471  \n",
      "118345  [-0.8882843, -0.42767513, -0.79361033, 0.70301...         10.000000  \n",
      "43645   [-0.94247055, -0.55554765, -0.962653, 0.850839...          8.880000  \n",
      "202960  [-0.4558781, -0.41420344, -0.87818205, 0.19026...          9.000000  \n",
      "\n",
      "[61359 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Annoy 인덱스 생성\n",
    "vector_length = len(X_train['book_summary_vector'].iloc[0])\n",
    "annoy_index = AnnoyIndex(vector_length, 'angular')\n",
    "\n",
    "# X_train의 book_summary_vector를 Annoy 인덱스에 추가\n",
    "for i, vector in enumerate(X_train['book_summary_vector']):\n",
    "    annoy_index.add_item(i, vector)\n",
    "\n",
    "# Annoy 인덱스 빌드\n",
    "annoy_index.build(10)  # 트리의 개수\n",
    "\n",
    "# X_valid에서만 존재하는 isbn에 대한 isbn_rating_mean을 대체\n",
    "def find_nearest_isbn_rating_mean(row):\n",
    "    if row['isbn'] in X_train['isbn'].values:\n",
    "        return X_train.loc[X_train['isbn'] == row['isbn'], 'isbn_rating_mean'].values[0]\n",
    "    else:\n",
    "        vector = row['book_summary_vector']\n",
    "        nearest_index = annoy_index.get_nns_by_vector(vector, 1)[0]\n",
    "        return X_train.iloc[nearest_index]['isbn_rating_mean']\n",
    "\n",
    "X_valid['isbn_rating_mean'] = X_valid.apply(find_nearest_isbn_rating_mean, axis=1)\n",
    "\n",
    "# 결과 출력\n",
    "print(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id    isbn                          user_summary_merge_vector  \\\n",
      "0         2719      39  [-0.74905014, -0.37544724, -0.58153605, 0.5005...   \n",
      "1        28656      39  [-0.26531896, -0.40486306, -0.97681594, 0.3710...   \n",
      "2        37434    4667  [-0.54214895, -0.4519513, -0.98219, 0.4661231,...   \n",
      "3        38564   32059  [-0.83539337, -0.3439815, -0.7150209, 0.616658...   \n",
      "4        16742   42311  [-0.7145952, -0.35694596, -0.9835796, 0.582363...   \n",
      "...        ...     ...                                                ...   \n",
      "76694    67993  126235  [-0.74315584, -0.4225567, -0.95433205, 0.62949...   \n",
      "76695    68001  141816  [-0.8367616, -0.5226269, -0.9474142, 0.7494029...   \n",
      "76696    68013  133571  [-0.76280177, -0.49358055, -0.93778366, 0.7140...   \n",
      "76697    68024  145168  [-0.83159566, -0.51806724, -0.9570092, 0.73350...   \n",
      "76698    68066   91703  [-0.60563767, -0.40665743, -0.72936785, 0.3261...   \n",
      "\n",
      "                                     book_summary_vector  isbn_rating_mean  \n",
      "0      [-0.47897604, -0.268497, -0.7921651, 0.3457237...          6.857143  \n",
      "1      [-0.47897604, -0.268497, -0.7921651, 0.3457237...          6.857143  \n",
      "2      [-0.84560686, -0.5757875, -0.98403776, 0.78440...          8.000000  \n",
      "3      [-0.861513, -0.5893982, -0.97968817, 0.7071627...          7.600000  \n",
      "4      [-0.5613843, -0.3075697, -0.8577777, 0.5032286...          7.571429  \n",
      "...                                                  ...               ...  \n",
      "76694  [-0.8767804, -0.51325154, -0.9722891, 0.737165...          8.000000  \n",
      "76695  [-0.802214, -0.5007278, -0.94391894, 0.7846305...          7.500000  \n",
      "76696  [-0.7942714, -0.5178748, -0.9568836, 0.6805196...          9.000000  \n",
      "76697  [-0.89846236, -0.4682132, -0.8941234, 0.770614...          6.000000  \n",
      "76698  [-0.71729314, -0.4043922, -0.96694696, 0.73474...          7.750000  \n",
      "\n",
      "[76699 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Annoy 인덱스 생성\n",
    "vector_length = len(X_for_train['book_summary_vector'].iloc[0])\n",
    "annoy_index = AnnoyIndex(vector_length, 'angular')\n",
    "\n",
    "# X_for_train의 book_summary_vector를 Annoy 인덱스에 추가\n",
    "for i, vector in enumerate(X_for_train['book_summary_vector']):\n",
    "    annoy_index.add_item(i, vector)\n",
    "\n",
    "# Annoy 인덱스 빌드\n",
    "annoy_index.build(10)  # 트리의 개수\n",
    "\n",
    "# X_test에서만 존재하는 isbn에 대한 isbn_rating_mean을 대체\n",
    "def find_nearest_isbn_rating_mean(row):\n",
    "    if row['isbn'] in X_for_train['isbn'].values:\n",
    "        return X_for_train.loc[X_for_train['isbn'] == row['isbn'], 'isbn_rating_mean'].values[0]\n",
    "    else:\n",
    "        vector = row['book_summary_vector']\n",
    "        nearest_index = annoy_index.get_nns_by_vector(vector, 1)[0]\n",
    "        return X_for_train.iloc[nearest_index]['isbn_rating_mean']\n",
    "\n",
    "X_test['isbn_rating_mean'] = X_test.apply(find_nearest_isbn_rating_mean, axis=1)\n",
    "\n",
    "# 결과 출력\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'isbn_rating_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/boostcamp11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'isbn_rating_mean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train_mean \u001b[38;5;241m=\u001b[39m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn_rating_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m X_valid_mean \u001b[38;5;241m=\u001b[39m \u001b[43mX_valid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43misbn_rating_mean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn_rating_mean\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m X_valid \u001b[38;5;241m=\u001b[39m X_valid\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn_rating_mean\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/boostcamp11/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/envs/boostcamp11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'isbn_rating_mean'"
     ]
    }
   ],
   "source": [
    "X_train_mean = X_train['isbn_rating_mean']\n",
    "X_valid_mean = X_valid['isbn_rating_mean']\n",
    "X_train = X_train.drop(['isbn_rating_mean'], axis=1)\n",
    "X_valid = X_valid.drop(['isbn_rating_mean'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_train_mean = X_for_train['isbn_rating_mean']\n",
    "X_test_mean = X_test['isbn_rating_mean']\n",
    "X_for_train = X_for_train.drop(['isbn_rating_mean'], axis=1)\n",
    "X_test = X_test.drop(['isbn_rating_mean'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_diff = y_valid - X_valid_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['user_summary_merge_vector', 'book_summary_vector'], axis=1)\n",
    "X_valid = X_valid.drop(['user_summary_merge_vector', 'book_summary_vector'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_train = X_for_train.drop(['user_summary_merge_vector', 'book_summary_vector'], axis=1)\n",
    "X_test = X_test.drop(['user_summary_merge_vector', 'book_summary_vector'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train, y_train_diff], axis=1)\n",
    "valid_df = pd.concat([X_valid, y_valid_diff], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_train = pd.concat([X_for_train, y_diff_for_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 열의 이름을 rating으로 변경\n",
    "train_df.columns = list(train_df.columns[:-1]) + ['rating']\n",
    "valid_df.columns = list(valid_df.columns[:-1]) + ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_train.columns = list(df_for_train.columns[:-1]) + ['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2719</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28656</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37434</td>\n",
       "      <td>4667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38564</td>\n",
       "      <td>32059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16742</td>\n",
       "      <td>42311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76694</th>\n",
       "      <td>67993</td>\n",
       "      <td>126235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76695</th>\n",
       "      <td>68001</td>\n",
       "      <td>141816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76696</th>\n",
       "      <td>68013</td>\n",
       "      <td>133571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76697</th>\n",
       "      <td>68024</td>\n",
       "      <td>145168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76698</th>\n",
       "      <td>68066</td>\n",
       "      <td>91703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76699 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id    isbn\n",
       "0         2719      39\n",
       "1        28656      39\n",
       "2        37434    4667\n",
       "3        38564   32059\n",
       "4        16742   42311\n",
       "...        ...     ...\n",
       "76694    67993  126235\n",
       "76695    68001  141816\n",
       "76696    68013  133571\n",
       "76697    68024  145168\n",
       "76698    68066   91703\n",
       "\n",
       "[76699 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        6.857143\n",
       "1        6.857143\n",
       "2        8.000000\n",
       "3        7.600000\n",
       "4        7.571429\n",
       "           ...   \n",
       "76694    8.000000\n",
       "76695    7.500000\n",
       "76696    9.000000\n",
       "76697    6.000000\n",
       "76698    7.750000\n",
       "Name: isbn_rating_mean, Length: 76699, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['user_id'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['isbn'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "train_dataset = RatingsDataset(train_df)\n",
    "valid_dataset = RatingsDataset(valid_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['user_id'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['isbn'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "train_dataset = RatingsDataset(df_for_train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        # User and item embeddings\n",
    "        user_embed = self.user_embedding(user)\n",
    "        item_embed = self.item_embedding(item)\n",
    "        \n",
    "        # User and item biases\n",
    "        user_bias = self.user_bias(user).squeeze()  # (batch_size,)\n",
    "        \n",
    "        # Inner product of embeddings\n",
    "        interaction = (user_embed * item_embed).sum(1)  # (batch_size,)\n",
    "        \n",
    "        # Final prediction with biases\n",
    "        prediction = interaction + user_bias \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(data['label2idx']['user_id'])\n",
    "num_items = len(data['label2idx']['isbn'])\n",
    "embedding_dim = 16  # 임베딩 차원 크기, 필요에 따라 조정\n",
    "\n",
    "model = MatrixFactorization(num_users, num_items, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 12.77199574251895\n",
      "Epoch 2, Training Loss: 5.867089594857713\n",
      "Epoch 3, Training Loss: 4.67196586861131\n",
      "Epoch 4, Training Loss: 4.452914923889517\n",
      "Epoch 5, Training Loss: 3.9122881164737775\n",
      "Epoch 6, Training Loss: 3.6800781438685877\n",
      "Epoch 7, Training Loss: 3.446075886574993\n",
      "Epoch 8, Training Loss: 3.281334041952341\n",
      "Epoch 9, Training Loss: 3.155237391982717\n",
      "Epoch 10, Training Loss: 3.0241582537373755\n",
      "Epoch 11, Training Loss: 2.945221598714304\n",
      "Epoch 12, Training Loss: 2.847526587070303\n",
      "Epoch 13, Training Loss: 2.7915063499409705\n",
      "Epoch 14, Training Loss: 2.6966823228309686\n",
      "Epoch 15, Training Loss: 2.662027607324176\n",
      "Epoch 16, Training Loss: 2.5969815198759463\n",
      "Epoch 17, Training Loss: 2.5730092145275663\n",
      "Epoch 18, Training Loss: 2.511145027170193\n",
      "Epoch 19, Training Loss: 2.49093701208339\n",
      "Epoch 20, Training Loss: 2.439613934079458\n",
      "Epoch 21, Training Loss: 2.421778993975584\n",
      "Epoch 22, Training Loss: 2.3718247465357267\n",
      "Epoch 23, Training Loss: 2.3687169763213354\n",
      "Epoch 24, Training Loss: 2.3157482997148695\n",
      "Epoch 25, Training Loss: 2.320964604181004\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 25  # 필요한 만큼 설정\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for user, item, rating in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = criterion(prediction, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test에 rating 열을 추가\n",
    "X_test['rating'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터에 대한 예측값 생성 (sample_submission.csv 사용)\n",
    "test_dataset = RatingsDataset(X_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "for user, item, _ in test_loader:\n",
    "    prediction = model(user, item)\n",
    "    predictions.extend(prediction.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission에 rating 열을 예측값으로 대체\n",
    "sample_submission['rating'] = predictions + X_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('./data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boostcamp11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
